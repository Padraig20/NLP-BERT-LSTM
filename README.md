# NLP Model Comparison Project

## Overview

This project investigates various machine learning techniques in the field of Natural Language Processing (NLP). It aims to compare the effectiveness of different traditional machine learning models with advanced deep learning approaches. The project uniquely positions baseline models like K-Nearest Neighbors (KNN), Decision Trees, Multi-Layer Perceptron (MLP), and Random Forest against deep learning architectures, including a simple LSTM and a BERT-based model, for NLP tasks.

## Models

### Baseline Models
- **K-Nearest Neighbors (KNN):** A simple yet effective algorithm used for classification tasks.
- **Decision Tree:** A tree-like model used to make decisions and predictions.
- **Multi-Layer Perceptron (MLP):** A basic form of a neural network used for testing fundamental NLP capabilities.
- **Random Forest:** An ensemble learning method for classification that operates by constructing multiple decision trees.

### Deep Learning Models
- **Simple LSTM:** An LSTM model to process text data in a sequential manner.
- **BERT Architecture:** A highly sophisticated model based on transformers, known for its effectiveness in understanding context and complex language patterns.
- **LSTM with BERT Embeddings:** Combines the LSTM's ability to model sequences with the rich contextual embeddings from BERT.

## Datasets

- **BBC Dataset:** A widely-used dataset for text classification, consisting of news articles categorized into various topics. This dataset provides a rich source of textual data for evaluating model performance in classifying content into predefined categories.
- **SMS Spam Collection Dataset:** A collection of SMS messages labeled as spam or ham (non-spam). This dataset presents a real-world challenge for models to accurately classify text messages, testing their ability to discern between spam and legitimate content.
